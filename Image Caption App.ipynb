{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.models import Model,Sequential,load_model\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tkinter import *\n",
    "from tkinter.filedialog import askopenfile\n",
    "from PIL import Image, ImageTk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "features_encoder=load_model('features_encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tokenizer of captions from the saved file \n",
    "file=open('tokenizer.pkl','rb')\n",
    "tokenizer=pickle.load(file)   \n",
    "\n",
    "#load the max length of captions from the saved file \n",
    "file=open('max_length.pkl','rb')\n",
    "max_length=pickle.load(file)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption_model=load_model('model1.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_feature(img_path):\n",
    "    #read each image by its path\n",
    "    image=cv2.imread(img_path)\n",
    "    #resize each image to (224,224)\n",
    "    image=cv2.resize(image,(224,224))\n",
    "    #convert each image to array\n",
    "    image=img_to_array(image)\n",
    "    #reshape each image \n",
    "    image=image.reshape((1,224,224,3))\n",
    "    #preprocess each image to inceptionV3 model\n",
    "    image=preprocess_input(image)\n",
    "    #predict the feature of each image\n",
    "    feature=features_encoder.predict(image,verbose=0) \n",
    "    feature_lst=[feature]\n",
    "    feature_arr=np.array(feature_lst)   \n",
    "    feature_arr=tf.reshape(feature_arr, (-1, 2048))\n",
    "    return feature_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer,tokenizer):\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "def predict_caption(model, feature, tokenizer, max_length):\n",
    "    in_text = \"startsen\"\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "\n",
    "        y_pred = model.predict([feature,sequence])\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        \n",
    "        word = idx_to_word(y_pred, tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        in_text+= \" \" + word\n",
    "        \n",
    "        if word == 'endsen':\n",
    "            break\n",
    "            \n",
    "    return in_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 1s 642ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n"
     ]
    }
   ],
   "source": [
    "root=Tk()\n",
    "root.title('Image Captioning Generator')\n",
    "root.config(background='gray')\n",
    "root.geometry('1024x1024+250+100')\n",
    "frame=Frame(root,bg='gray')\n",
    "def open_image():\n",
    "    empty_lbl=Label(frame,text=200*\" \"\n",
    "              ,height='8',bg='gray',fg='white',font='25')\n",
    "    empty_lbl.grid(column=1,row=3,padx=10, pady=10)\n",
    "    img_path = askopenfile(mode='r', filetypes=[('Image Files', '*.jpg *.png *.jpeg')])\n",
    "    if img_path:\n",
    "        #image = Image.open(img_path)\n",
    "        #photo = ImageTk.PhotoImage(image)\n",
    "        #label.config(image=photo)\n",
    "        #label.image = photo\n",
    "        # Place the label at specific coordinates\n",
    "        #label.place(x=50, y=50)\n",
    "        feature=predict_feature(str(img_path.name))\n",
    "        cap=predict_caption(image_caption_model,feature,tokenizer,max_length)\n",
    "        cap_lbl=Label(frame,text=cap\n",
    "              ,height='3',bg='black',fg='white',font='25')\n",
    "        cap_lbl.grid(column=1,row=3,padx=10, pady=10)\n",
    "\n",
    "        \n",
    "lbl=Label(frame,text='Display the image and its caption'\n",
    "              ,height='3',bg='gray',fg='black',font='25')\n",
    "lbl.grid(column=1,row=1,padx=10, pady=10)\n",
    "browse=Button(frame, text=\"Select image\", command=open_image,width='25',height='3',font='30',bg='white')\n",
    "browse.grid(column=1,row=2,padx=10, pady=10)\n",
    "\n",
    "frame.pack()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
